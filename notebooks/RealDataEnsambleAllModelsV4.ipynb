{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "095690ab-31ea-41b1-8298-9b772de14270",
   "metadata": {},
   "source": [
    "# **Anomaly Detection in Time Series Data using Ensemble Models**\n",
    "\n",
    "This notebook demonstrates how to build and evaluate individual anomaly detection models and then create an ensemble of these models for improved performance.\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "1. [Data Preparation](#data-preparation)\n",
    "2. [Build and Evaluate Individual Models](#individual-models)\n",
    "   - [Matrix Profile](#matrix-profile)\n",
    "   - [Isolation Forest](#isolation-forest)\n",
    "   - [Autoencoder](#autoencoder)\n",
    "   - [Convolutional Neural Network (CNN)](#cnn)\n",
    "3. [Build and Evaluate Ensemble Model](#ensemble-model)\n",
    "4. [Analyze Results](#analyze-results)\n",
    "5. [Optimize Models and Ensemble](#optimize-models)\n",
    "6. [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ccd4b52-1ba5-47a2-8850-11cc484b7acb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'preprocess_train_data' from 'commons' (C:\\Users\\b50578\\OneDrive - FH CAMPUS 02\\Documents\\PhD\\ProjectsSandbox\\Ensemble\\commons.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     13\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_sequences, load_data, add_advanced_time_features, add_lag_features, filter_and_save_data, scale_data, scale_selected_columns, preprocess_train_data, preprocess_eval_data, plot_anomalies\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m timesteps, data_file_path, time_column, target_column, train_data_split, train_data_path, eval_data_path, anomaly_trashold\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'preprocess_train_data' from 'commons' (C:\\Users\\b50578\\OneDrive - FH CAMPUS 02\\Documents\\PhD\\ProjectsSandbox\\Ensemble\\commons.py)"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from commons import create_sequences, load_data, add_advanced_time_features, add_lag_features, filter_and_save_data, scale_data, scale_selected_columns, preprocess_train_data, preprocess_eval_data, plot_anomalies\n",
    "from commons import timesteps, data_file_path, time_column, target_column, train_data_split, train_data_path, eval_data_path, anomaly_trashold   # Variables which are shared across the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e9b86-c711-485f-90e5-24893c0a8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction_error(data, time_column, reconstruction_error, threshold, title=\"Reconstruction Error with Anomalies\"):\n",
    "    \"\"\"\n",
    "    Plots the reconstruction error over time and highlights points exceeding the anomaly threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: pd.DataFrame containing the dataset\n",
    "    - time_column: str, name of the time column\n",
    "    - reconstruction_error: np.array, reconstruction error values\n",
    "    - threshold: float, anomaly threshold\n",
    "    - title: str, plot title\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    # Plot the reconstruction error over time\n",
    "    ax.plot(data[time_column], reconstruction_error, label='Reconstruction Error', color='blue')\n",
    "\n",
    "    # Highlight anomalies (points where reconstruction error exceeds the threshold)\n",
    "    anomalies = data[reconstruction_error > threshold]\n",
    "    ax.scatter(anomalies[time_column], reconstruction_error[reconstruction_error > threshold], \n",
    "               color='red', label='Anomalies', marker='x')\n",
    "\n",
    "    # Add threshold line\n",
    "    ax.axhline(y=threshold, color='r', linestyle='--', label='Threshold')\n",
    "\n",
    "    # Add title and labels\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Reconstruction Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_combined_anomalies_only(data, time_column, value_column, predictions, title=\"Anomalies Detected\"):\n",
    "    \"\"\"\n",
    "    Plots the data with anomalies highlighted.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: pd.DataFrame containing the dataset\n",
    "    - time_column: str, name of the time column\n",
    "    - value_column: str, name of the value column (e.g., power output)\n",
    "    - predictions: np.array, combined predictions from both models (1 for normal, -1 for anomaly)\n",
    "    - title: str, plot title\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    # Plot the actual data values over time\n",
    "    ax.plot(data[time_column], data[value_column], label='Data', color='blue')\n",
    "\n",
    "    # Highlight anomalies in red\n",
    "    anomalies = data[predictions == -1]\n",
    "    ax.scatter(anomalies[time_column], anomalies[value_column], color='red', label='Anomalies', marker='x')\n",
    "\n",
    "    # Add title and labels\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(value_column)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_combined_anomalies(data, time_column, value_column, ensemble_type, title=None):\n",
    "    \"\"\"\n",
    "    Plots the data and highlights anomalies detected by different models whose predictions are stored in the DataFrame columns.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pd.DataFrame, the original data with time and value columns, and prediction columns\n",
    "    - time_column: str, name of the time column\n",
    "    - value_column: str, name of the column with values to plot\n",
    "    - ensemble_type: str, the type of ensemble method used\n",
    "    - title: str or None, title of the plot. If None, a default title is generated\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Ensure the time column is in datetime format\n",
    "    data[time_column] = pd.to_datetime(data[time_column])\n",
    "\n",
    "    # Set default title if not provided\n",
    "    if title is None:\n",
    "        title = f\"Anomalies Detected by Ensemble with {ensemble_type}\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    # Plot the original data\n",
    "    ax.plot(data[time_column], data[value_column], label='Original Data', color='blue')\n",
    "\n",
    "    # Define colors and markers\n",
    "    colors = ['orange', 'red', 'green', 'purple', 'cyan', 'magenta']\n",
    "    markers = ['x', 'o', '^', 's', 'd', '*']\n",
    "\n",
    "    # Get prediction columns dynamically\n",
    "    prediction_columns = [col for col in data.columns if col.endswith('_prediction')]\n",
    "\n",
    "    for idx, pred_col in enumerate(prediction_columns):\n",
    "        anomalies = data[data[pred_col] == -1]\n",
    "        model_label = pred_col.replace('_prediction', '').replace('_', ' ').title() + ' Anomalies'\n",
    "        ax.scatter(\n",
    "            anomalies[time_column], anomalies[value_column],\n",
    "            color=colors[idx % len(colors)],\n",
    "            marker=markers[idx % len(markers)],\n",
    "            label=model_label\n",
    "        )\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(value_column)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_models(models_info):\n",
    "\n",
    "    models = {}\n",
    "    scalers = {}\n",
    "\n",
    "    for model_name, info in models_info.items():\n",
    "        model_path = info['path']\n",
    "        model_type = info['type']\n",
    "        scaler_path = info.get('scaler')\n",
    "\n",
    "        # Load the model\n",
    "        if model_type == 'sklearn':\n",
    "            models[model_name] = joblib.load(model_path)\n",
    "        elif model_type == 'keras':\n",
    "            models[model_name] = load_model(model_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "        # Load the scaler if it exists\n",
    "        if scaler_path:\n",
    "            scalers[model_name] = joblib.load(scaler_path)\n",
    "        else:\n",
    "            scalers[model_name] = None\n",
    "    return models, scalers\n",
    "\n",
    "\n",
    "def scale_selected_columns(data, feature_columns_to_scale, scaler):\n",
    "    \"\"\"\n",
    "    Scales only the specified columns in the dataset using the provided scaler.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The dataset with features to scale.\n",
    "    - feature_columns_to_scale (list): List of column names to be scaled.\n",
    "    - scaler (object): An instance of a scaler (e.g., StandardScaler or MinMaxScaler).\n",
    "\n",
    "    Returns:\n",
    "    - data (pd.DataFrame): The dataset with only the specified columns scaled.\n",
    "    \"\"\"\n",
    "    # Copy the dataset to avoid modifying the original data\n",
    "    data_scaled = data.copy()\n",
    "\n",
    "    # Apply the scaler to the selected columns\n",
    "    data_scaled[feature_columns_to_scale] = scaler.transform(data[feature_columns_to_scale])\n",
    "\n",
    "    return data_scaled\n",
    "\n",
    "\n",
    "\n",
    "def make_predictions(models, preprocessed_data, data, ocsvm_threshold=anomaly_trashold):\n",
    "    \"\"\"\n",
    "    Makes predictions using each model and stores the predictions in the data DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - models: dict, contains loaded models\n",
    "    - preprocessed_data: dict, contains preprocessed data for each model\n",
    "    - data: pd.DataFrame, the original data where predictions will be stored\n",
    "    - ocsvm_threshold: float, percentile threshold for OneClassSVM anomaly detection (default: 95th percentile)\n",
    "\n",
    "    Returns:\n",
    "    - data: pd.DataFrame, the data with predictions added\n",
    "    \"\"\"\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        prediction_col = f\"{model_name}_prediction\"\n",
    "\n",
    "        # Make predictions based on model type\n",
    "        if isinstance(model, (IsolationForest, OneClassSVM)):\n",
    "            # For OneClassSVM, calculate decision function (anomaly scores) and apply dynamic threshold\n",
    "            if isinstance(model, OneClassSVM):\n",
    "                # Calculate decision function (anomaly scores)\n",
    "                anomaly_scores = -model.decision_function(preprocessed_data)\n",
    "                # Set a dynamic threshold based on the provided percentile\n",
    "                threshold = np.percentile(anomaly_scores, ocsvm_threshold)\n",
    "                # Label anomalies: -1 for anomaly, 1 for normal\n",
    "                predictions = np.where(anomaly_scores > threshold, -1, 1)\n",
    "            else:\n",
    "                # For IsolationForest, we directly use the model's predictions (-1 for anomaly, 1 for normal)\n",
    "                predictions = model.predict(preprocessed_data)\n",
    "\n",
    "        elif isinstance(model, keras.Model):\n",
    "          \n",
    "            \n",
    "            # Check if the model is deep_autoencoder or an seq_autoencoder based on its name\n",
    "            if \"autoencoder_deep\" in model_name.lower():\n",
    "                reconstructed_data = model.predict(preprocessed_data)  # Use non-sequence data if that’s how it was trained.\n",
    "                reconstruction_error = np.mean(np.abs(preprocessed_data - reconstructed_data), axis=1)\n",
    "                threshold = np.percentile(reconstruction_error, anomaly_trashold)\n",
    "                predictions = np.where(reconstruction_error > threshold, -1, 1)\n",
    "               \n",
    "            elif \"autoencoder_seq\" in model_name.lower():\n",
    "\n",
    "                if preprocessed_data.shape[0] >= timesteps:\n",
    "                    preprocessed_data_seq  = create_sequences(preprocessed_data, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(\"Not enough samples to create the required sliding windows.\")\n",
    "                # For Autoencoder, compute reconstruction error\n",
    "                reconstructed_data = model.predict(preprocessed_data_seq)\n",
    "\n",
    "                                \n",
    "                if reconstructed_data.shape != preprocessed_data_seq.shape:\n",
    "                    # The autoencoder returns only one timestep per sequence.\n",
    "                    print(\"Using sequence-to-one error calculation (comparing last time step)\")\n",
    "                    # Compare only the last time step in each sequence with the model output.\n",
    "                    reconstruction_error = np.mean(np.abs(preprocessed_data_seq[:, -1, :] - reconstructed_data), axis=1)\n",
    "                else:\n",
    "                    print(\"Using full sequence error calculation\")\n",
    "                    reconstruction_error = np.mean(np.abs(preprocessed_data_seq - reconstructed_data), axis=(1, 2))\n",
    "                \n",
    "                #reconstruction_error = np.mean(np.abs(preprocessed_data_seq - reconstructed_data), axis=(1, 2))\n",
    "                # Set a dynamic threshold (e.g., 95th percentile)\n",
    "                threshold = np.percentile(reconstruction_error, anomaly_trashold)\n",
    "                # Label anomalies: -1 for anomaly, 1 for normal\n",
    "                predictions_seq = np.where(reconstruction_error > threshold, -1, 1)\n",
    "                \n",
    "                # Create a full-length predictions array and pad the first (timesteps - 1) rows.\n",
    "                predictions = np.ones(preprocessed_data.shape[0], dtype=int)  # default to 1 (normal)\n",
    "                predictions[timesteps - 1:] = predictions_seq\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model instance: {model}\")\n",
    "\n",
    "        # Store predictions in the DataFrame\n",
    "        data[prediction_col] = predictions\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06071b91-f9cd-4c00-8876-aa64dceb9de9",
   "metadata": {},
   "source": [
    "<a id='ensemble-model'></a>\n",
    "##  Build and Evaluate Ensemble Model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36afedb2-272e-4e8b-a950-7e2b18311a3e",
   "metadata": {},
   "source": [
    "## Load all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b124b2e-8824-4300-946b-b308ce814f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the models dictionary\n",
    "models_info = {\n",
    "    'iso_forest': {\n",
    "        'path': f'USModels/isolation_forest_model_{timesteps}.pkl',\n",
    "        'type': 'sklearn',\n",
    "        'scaler': f'USModels/scaler_iso_forest_{timesteps}.pkl'\n",
    "        #'scaler': None\n",
    "    },\n",
    "    'autoencoder_seq': {\n",
    "        'path': f'USModels/best_seq_autoencoder_model_{timesteps}.keras',\n",
    "        'type': 'keras',\n",
    "        'scaler': f'USModels/scaler_ae_{timesteps}.pkl'\n",
    "    },\n",
    "#    'lstm': {\n",
    " #       'path': f'USModels/lstm_forecaster_model_{timesteps}.keras',\n",
    "#        'type': 'keras',\n",
    "#        'scaler': f'USModels/scaler_ae_{timesteps}.pkl'\n",
    "#    },\n",
    "#    'autoencoder_deep': {\n",
    "#        'path': f'USModels/best_deep_autoencoder_model_{timesteps}.keras',\n",
    "#        'type': 'keras',\n",
    "#        'scaler': f'USModels/scaler_ae_{timesteps}.pkl'\n",
    "#   },\n",
    "    'oc_svm': {\n",
    "        'path': f'USModels/ocsvm_model_{timesteps}.pkl',\n",
    "        'type': 'sklearn',\n",
    "        'scaler': f'USModels/scaler_ocsvm_{timesteps}.pkl'\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. Load the models and scalers dynamically\n",
    "models, scalers = load_models(models_info)\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd333eb-fbec-44c7-a1ca-4a635b6da513",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_x_days_data = preprocess_eval_data(\n",
    "    eval_data_path=eval_data_path, \n",
    "    time_column=time_column, \n",
    "    target_column=target_column, \n",
    "    timesteps=timesteps, \n",
    "    scaler=scalers['oc_svm'] # the scaler is the sam for all\n",
    ")\n",
    "# Drop the 'Time' column before fitting the model, as IsolationForest only accepts numeric features\n",
    "last_x_days_data_numeric = last_x_days_data.drop(columns=['Time'])\n",
    "print(last_x_days_data_numeric.head())\n",
    "\n",
    "\n",
    "# 5. Make predictions with each model\n",
    "last_x_days_data = make_predictions(models, last_x_days_data_numeric, last_x_days_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f987a-670a-4f9c-a54e-20e66014a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Combine predictions\n",
    "# For ensemble methods, you can dynamically retrieve the predictions\n",
    "prediction_columns = [col for col in last_x_days_data.columns if col.endswith('_prediction')]\n",
    "\n",
    "# Convert predictions to numerical values (-1 for anomaly, 1 for normal)\n",
    "predictions_matrix = np.column_stack([last_x_days_data[col].values for col in prediction_columns])\n",
    "\n",
    "# Extract model names from prediction columns by removing the '_prediction' suffix\n",
    "model_names = [col.replace('_prediction', '') for col in prediction_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4195b7fe-6a73-424b-bbbf-51e03349f038",
   "metadata": {},
   "source": [
    "### Either using the majority voting 1st example or weighted averaging 2ond example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c763021a-5fa0-4a0e-94d2-3b703cce2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority Voting\n",
    "sum_predictions = np.sum(predictions_matrix, axis=1)\n",
    "combined_predictions_majority = np.where(sum_predictions <= -1, -1, 1)  # At least half models predict anomaly\n",
    "\n",
    "\n",
    "# Weighted Averaging (Define your own weights)\n",
    "#weights = np.full(len(prediction_columns), 1 / len(prediction_columns))  # Equal weights\n",
    "# Define your weights for each model\n",
    "\n",
    "weights_dict = {\n",
    "    'iso_forest': 0.4,\n",
    "    'autoencoder_seq': 0.4,\n",
    "    'lstm': 0.3,  # Added weight for lstm\n",
    "    'autoencoder_deep': 0.4,\n",
    "    'oc_svm': 0.2\n",
    "}\n",
    "weights = np.array([weights_dict[model_name] for model_name in model_names])\n",
    "\n",
    "\n",
    "combined_scores = np.dot(predictions_matrix, weights)\n",
    "combined_predictions_weighted = np.where(combined_scores < 0, -1, 1)\n",
    "\n",
    "# Add combined predictions to the DataFrame\n",
    "ensemble_type = 'Majority Voting'  # 'Weighted Averaging', 'Stacking', etc.\n",
    "#ensemble_type = 'Weighted Averaging'  # or 'Weighted Averaging'\n",
    "if ensemble_type == 'Majority Voting':\n",
    "    last_x_days_data['combined_prediction'] = combined_predictions_majority\n",
    "else:\n",
    "    last_x_days_data['combined_prediction'] = combined_predictions_weighted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a703a-ef73-41f0-a6c4-d2c5bbbe4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_anomalies = (combined_predictions_weighted == -1).sum()\n",
    "num_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0739d3b-b63b-4dbd-911c-fbeb59cbb849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming anomalies are marked as -1 in 'combined_prediction'\n",
    "num_anomalies = (last_x_days_data['combined_prediction'] == -1).sum()\n",
    "\n",
    "# Print the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {num_anomalies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe244c9-2550-4ed8-b19c-5d9c2b661511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, simply call the function without passing predictions\n",
    "plot_combined_anomalies(\n",
    "    data=last_x_days_data,\n",
    "    time_column='Time',\n",
    "    value_column='Inv 1 AC-Leistung (W)',\n",
    "    ensemble_type=ensemble_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce759ed4-8645-44b5-b33f-730e5dc81cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot combined anomalies\n",
    "plot_combined_anomalies_only(\n",
    "    data=last_x_days_data,                  # The DataFrame containing the dataset\n",
    "    time_column='Time',                     # The name of the time column\n",
    "    value_column='Inv 1 AC-Leistung (W)',   # The value column (e.g., power output)\n",
    "    predictions=last_x_days_data['combined_prediction'].values,  # Combined predictions (1 for normal, -1 for anomaly)\n",
    "    title=\"Detected Anomalies from ensamble in Power Output\"  # Plot title (optional)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f042d0-3402-4d35-b813-7db25c8db20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of prediction method column names\n",
    "\n",
    "prediction_methods = [\"iso_forest_prediction\", \n",
    "                      \"autoencoder_seq_prediction\", \n",
    "                      \"oc_svm_prediction\", \n",
    "                      \"combined_prediction_majority\", \n",
    "                      \"combined_prediction_union\", \n",
    "                      \"combined_prediction_weighted\", \n",
    "                      \"combined_prediction_stacking_logistic_reg\", \n",
    "                      \"combined_prediction_stacking_random_for\",\n",
    "                      \"combined_prediction_stacking_xgboost\"]\n",
    "\n",
    "# Loop through each prediction method and plot the anomalies\n",
    "for method in prediction_methods:\n",
    "    # Check if the current prediction column exists in the DataFrame\n",
    "    if method in last_x_days_data.columns:\n",
    "        # Create a dynamic title by formatting the method name\n",
    "        title = (\n",
    "            f\"Detected Anomalies from {method.replace('_prediction', '').replace('_', ' ').title()} in Power Output\"\n",
    "        )\n",
    "        # Call the plotting function with the dynamic predictions column\n",
    "        plot_combined_anomalies_only(\n",
    "            data=last_x_days_data,                  # The DataFrame containing the dataset\n",
    "            time_column='Time',                     # The name of the time column\n",
    "            value_column='Inv 1 AC-Leistung (W)',   # The value column (e.g., power output)\n",
    "            predictions=last_x_days_data[method].values,  # Dynamically use the prediction values\n",
    "            title=title                             # Dynamic title for the plot\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Column '{method}' not found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43123330-af07-4ddc-9012-9034dcfc9c21",
   "metadata": {},
   "source": [
    "## Archive code dependent on labels which I dont have for unsupervized learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b35f677-8f53-40a3-8c9e-5b344a1edce5",
   "metadata": {},
   "source": [
    "<a id='analyze-results'></a>\n",
    "## **4. Analyze Results**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeedbcd-6337-4dd1-91bd-62bbc65ddc24",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## **6. Conclusion**\n",
    "\n",
    "- **Optimized Individual Models**: Hyperparameter tuning was performed on Matrix Profile, Autoencoder, and CNN models.\n",
    "- **Improved Performance**: Each model showed performance improvements after optimization.\n",
    "- **Enhanced Ensemble Model**: The ensemble model was updated with optimized models, leading to better overall performance.\n",
    "- **Weighted Ensemble**: Assigning weights based on individual model performance further improved the ensemble's effectiveness.\n",
    "- **Future Work**: Further optimization can be done using more sophisticated techniques like Bayesian Optimization, and additional models can be incorporated into the ensembleparameters.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
